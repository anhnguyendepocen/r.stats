---
title:
author: "cjlortie"
date: "May 2016"
output:
  html_document:
    theme: yeti
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
---
#BIOL5081: Biostatistics Intro
![](./stats.JPG)
The first six weeks of the course will cover data science & fundamental exploratory data analysis in r.  It is a brief introduction to the contemporary open science, best-practice data and biostatistical working tools. The primary goal is exploration of tools and approaches associated with effective, efficient, reproducible biostatistical analyses. Inspirations include [software carpentry](http://software-carpentry.org), [rforcats](http://rforcats.net), and many, more open, online, free I can direct you to as needed. The [description](http://biology.gradstudies.yorku.ca/courses/biol-5081/) provided by the university is a useful starting point in deciding if the Fall 2016 offering meets your specific needs.

use.ful

bio.stats

adventure.time


[Course outline](https://github.com/cjlortie/r.stats/blob/gh-pages/BIOL5081-Fall2016.pdf)



###Lesson 1. Data science (DS).
![](./bubblegum.png)
want data. have data. will collect data.
assumption: in this course, you are here to work with data.
data literacy IS data science.


[WhyR introductory lecture](http://www.slideshare.net/cjlortie/whyr)

[The importance of data viz](http://datascienceplus.com/the-importance-of-data-visualization/)

<b> Philosophy of R stats </b>
Statistical thinking: likelihood, error, and effect sizes. Contemporary statisticians embrace and are mindful of these three key concepts in all research, data, and statistical inference examinations.

Modes of inference with your data: using data, what can you infer or do?
1. Data description
2. Likelihood
3. Estimation
4. Baysian inference - weight and include what we know
5. Prediction
6. Hypothesis testing
7. Decision making -balance gains and risks

This set of ideas provide the foundation for many data science and statistical approached to working with evidence within almost every domain of science.

<b>Data viz first and foremost. Case study #1.</b>
```{r, data viz exercise}
#blog post by Fisseha Berhane
library(ggplot2)
library(dplyr)
library(reshape2)

#Create four groups: setA, setB, setC and setD.
setA=select(anscombe, x=x1,y=y1)
setB=select(anscombe, x=x2,y=y2)
setC=select(anscombe, x=x3,y=y3)
setD=select(anscombe, x=x4,y=y4)

#Add a third column which can help us to identify the four groups.
setA$group ='SetA'
setB$group ='SetB'
setC$group ='SetC'
setD$group ='SetD'

#merge the four datasets
all_data=rbind(setA,setB,setC,setD)  # merging all the four data sets
all_data[c(1,13,23,43),]  # showing sample

#compare summary stats
summary_stats =all_data%>%group_by(group)%>%summarize("mean x"=mean(x),
                                       "Sample variance x"=var(x),
                                       "mean y"=round(mean(y),2),
                                       "Sample variance y"=round(var(y),1),
                                       'Correlation between x and y '=round(cor(x,y),2)
                                      )
models = all_data %>% 
      group_by(group) %>%
      do(mod = lm(y ~ x, data = .)) %>%
      do(data.frame(var = names(coef(.$mod)),
                    coef = round(coef(.$mod),2),
                    group = .$group)) %>%
dcast(., group~var, value.var = "coef")

summary_stats_and_linear_fit = cbind(summary_stats, data_frame("Linear regression" =
                                    paste0("y = ",models$"(Intercept)"," + ",models$x,"x")))

summary_stats_and_linear_fit

#data viz instead as first step
ggplot(all_data, aes(x=x,y=y)) +geom_point(shape = 21, colour = "red", fill = "orange", size = 3)+
    ggtitle("Anscombe's data sets")+geom_smooth(method = "lm",se = FALSE,color='blue') + 
    facet_wrap(~group, scales="free")

```
Outcome from stats first, data viz later (tricked), descriptive estimates of data can be deceptive. Draw first, then interpret.


<b>Survey data from class. Case study #2.</b>
```{r, survey}
#load class survey responses from google poll completed in class
survey<-read.csv("data/5081.survey.1.csv")
str(survey) #check data match what we collected

#data viz
hist(survey$r.experience, xlab="experience in R (1 is none, 5 is extensive)", ylab="frequency", main="Likert Scale 1 to 5")
plot(survey$r.experience~survey$discipline, xlab="discipline", ylab="experience in R")
plot(survey$r.studio, ylab="R Studio")
plot(survey$research.data, ylab="Research data")
#observe patterns by checking plots
```
<b>Observations from data viz</b>
We have limited experience in R. Experience in R varies by research discipline. A total of half the respondents have tried R Studio. Most participants will be working with quantitative data in their own research projects.

```{r, test survey data with EDA}
#Now, try some simple summary statistics.
summary(survey)
#Data summary looks reasonable based on plots, mean R experience is < 2
t.test(survey$r.experience, mu=1) #t-test if mean is different from 1
t.test(survey$r.experience, mu=2) #t-test if mean is different from 2
#A one sample t-test confirms we have a bit experience in R.

m1<-glm(r.experience~discipline, family = poisson, data = survey) #test for differenes between disciplines in R experience
m1 #model summary
anova(m1, test="Chisq") #test whether the differences in model are different
#Too little evidence to be significantly different between disciplines.

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Understand the difference between R and R Studio.
Use scripts or R Markdown files to save all your work.
Be prepared to share you code.
Load data, clean data, visualize data, then and only then begin applying statistics.
Proximately: be able to use and work with dataframes, vectors, functions, and libraries in R.

###Lesson 2. Workflows & Data Wrangling (WDW).
![](./beemo.rodeo.png)
worflows
reproduce. 
openly.

data wrangling
more than half the battle.


[Data wrangling slide deck](http://www.slideshare.net/cjlortie/data-wrangling-in-r-be-a-wrangler)

<b> Philosophy of R stats </b>
Tidy data make your life easier. Data strutures should match intuition and common sense. Data should have logical structure.  Rows are are observations, columns are variables. Tidy data also increase the viability that others can use your data, do better science, reuse science, and help you and your ideas survive and thrive. A workflow should also include the wrangling you did to get your data ready. If data are already very clean in a spreadsheet, they can easily become a literate, logical dataframe. Nonetheless, you should still use annotation within the introductory code to explain the meta-data of your data to some extent and what you did pre-R to get it tidy.  The philosophy here is very similar to the data viz lesson forthcoming with two dominant paradigms. Base R code functions, and pipes %>% and the logic embodied within the libraries associated with the the tidyverse. Generally, I prefer the tidyverse because it is more logical and much easier to remember.  It also has some specific functions needed to address common errors in modern data structures with little code needed to fix them up.

<b>Worflow</b>
template for r-script
meta-data ####
author: cjlortie
date: 2016
purpose: 

set-up ####
rm(list=ls())
getwd()
setwd("~/r")

read & check data ####
data <-read.csv(“filename.csv”)
names()
dim()
str()
summary(data)

visualize ####

check assumptions ####

model data and test hypothesis ####

<b>Data wrangling</b>

<b>Base R</b>
key concepts:
aggregate
tapply
sapply 
lappy
subsetting
as.factor
is.numeric
na

<b>tidyverse</b>
key concepts:
pipes are you best friend!
%>% 

dplyr
filter for rows
select for columns
mutate for new variables
summarise for bringing together many values

[Excellent list of wrangling tools](http://www.computerworld.com/article/2921176/business-intelligence/great-r-packages-for-data-import-wrangling-visualization.html)


[Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)

[tidyr](https://cran.r-project.org/web/packages/tidyr/README.html)

[Great wrangling webinar](https://www.rstudio.com/resources/webinars/data-wrangling-with-r-and-rstudio/)

We will cover two basic challenges you will certainly encounter.
<b>Missing data </b>
```{r, missing data}
#Missing data. In error, missing cells/observations in some measure can kick back an error. In other apps, sometimes ignored but can introduce error.
ttc <-read.csv("data/ttc.csv")
str(ttc)

#check for missing values
is.na(ttc)
summary(ttc, na.rm=TRUE) #excludes NA
new.ttc <-na.omit(ttc) # returns without missing values
is.na(new.ttc) #check to see if it worked

#many other solutions but I use these two frequently

```

<b>Selecting part of a dataset </b>
```{r, selections}
library(dplyr)
survey<-read.csv("data/5081.survey.1.csv")
str(survey)

#I want just a simple dataframe with experience by discipline

experience <- survey %>% select(discipline, r.experience)
experience

#Now I just want to select the physiology folks
physiologists <- experience %>% filter(discipline == "physiology")
physiologists

#Selections also often include a summary by levels or I want to make a new column with some calculations. Think about what you have likely done in excel.

#used pipes and made a nice summary table
experience <-survey %>% group_by(discipline) %>% summarise(
  count = n(),
  exp = mean (r.experience)
)

#What if I just want to make a new column to my dataframe that is a sum, a calculation, or some addition
ttc.5.years <- ttc %>% mutate(five.year.sum = X2015+X2014+X2013+X2012+X2011)
str(ttc.5.years)
str(ttc)

#so we made a new column.
#can do this to original dataframe too without making new object
ttc %>% mutate(five.year.sum = X2015+X2014+X2013+X2012+X2011)
ttc
#notice any errors? :)

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Check and address missing values.
Grab part of a dataset.
Use pipes to move/wrangle chunks of your dataset

###Lesson 3. Visualization in r (VR).
![](./viz.png)
basic plots.
lattice.
ggplot2.
you need to see data. see the trends. explore them using visuals.

[Contemporary data viz for statistical analyses slide deck](http://www.slideshare.net/cjlortie/data-visualization-in-r-65991145)

<b> Philosophy of R stats </b>
Clean simple graphics are powerful tools in statistics (and in scientific communication).  Tufte and others have shaped data scientists and statisticians in developing more libraries, new standards, and assumptions associated with graphical representations of data.  Data viz must highlight the differences, show underlying data structures, and provide insights into the specific research project. R is infinitely customizable in all these respects.  There are at least two major current paradigms (there are more these are the two dominant idea sets).  Base R plots are simple, relatively flexible, and very easy. However, their grammar, i.e their rules of coding are not modern. Ggplot and related libraries invoke a new, formal grammar of graphics that is more logical, more flexible, but divergent from base R code. It is worth the time to understand the differences and know when to use each.

Evolution of plotting in statistics using R in particular went from base-R then onto lattice then to the ggvis universe with the most recent library being ggplot2. Base-R is certainly useful in some contexts as is the lattice and lattice extra library. However, ggplot2 now encompasses all these capacities with a much simpler set of grammar (i.e. rules and order). Nonetheless, you should be able to read base-R code for plots and be able to do some as well. The philosophy or grammar of modern graphics is well articulated and includes the following key principles.

The grammar of graphics
layers
primacy of layers (simple first, then more complex) i.e. you build up your plots
data are mapped to aesthetic attributes and geometric objects
data first then statistics even in plots

Disclaimer: I love the power of qplots.

[ggplot2 cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)

<b>Data viz case study #1.</b>
```{r, survey data viz}
library(ggplot2)
survey<-read.csv("data/5081.survey.1.csv")
str(survey)

plot(survey$r.experience) #hard to tell what is going on

qplot(r.experience, data=survey) #decided to make bins for me and count up)

#so, we know better and instead do a histogram using base graphics
#basic data viz for EDA
hist(survey$r.experience) #better
qplot(r.experience, data=survey, geom="histogram") #same as what is picked for us
qplot(r.experience, data=survey, geom="histogram", binwidth=0.5)

barplot(survey$r.experience) #confusing
qplot(r.experience, data=survey, geom="bar") #what, it is back!

#basic data viz for EDA but for interactions
plot(survey$discipline, survey$r.experience)
qplot(discipline, r.experience, data=survey) #not the same
qplot(discipline, r.experience, data=survey, geom="boxplot")

plot(survey$r.studio~survey$r.experience) #ugly
qplot(r.experience, r.studio, data=survey) #useless
qplot(r.studio, data=survey, weight = r.experience) #sweet new feature here

#ok, so you get it. grammar different, visuals about the same for super quick, simple plots. The grammar hints at the power that awaits though.

#grammar different, simple x or x~y plots about the same

```

<b>Data viz case study #2.</b>
```{r, diamonds are our best friend}
str(diamonds)
#crazy number of observations. We need less. too many riches not always good.
set.seed(1000)
dsmall<-diamonds[sample(nrow(diamonds), 100), ]

plot(dsmall$carat, dsmall$price)
qplot(carat, price, data=dsmall)

#ok no difference
#now let's see what we can do with qplot with a few bits added
#one little argument extra added
qplot(carat, price, data = dsmall, colour = color)
qplot(carat, price, data = dsmall, shape = cut)

#how about using data viz to even more thoroughly explore potential stats we could do.
#qplots - quick plot, thoughful build of layers
qplot(carat, price, data = dsmall, geom = c("point", "smooth"))

#what about trying some stats on this now, at least from a viz philosophy
qplot(color, price / carat, data = dsmall, geom = "boxplot") #can include formulas and methods

#or check for proportional differences
qplot(carat, data = dsmall, geom = "histogram", fill = color) #to see proportions
qplot(carat, data = dsmall, geom = "histogram", weight = price) # weight by a covariate
     
#final idea, how about subsetting with the data right in the code for the plots!
qplot(carat, data = diamonds, facets = color ~ .,
  geom = "histogram", binwidth = 0.1, xlim = c(0, 3)) #to compare between groups

#qplot is so powerful.
#colour, shape and size have meaning in the R code from this library
#layers added for you by qplots

#qplot gets you 2x and one y or one x and 2y so >2 variables at once easily
```

<b>Data viz case study #3.</b>
```{r, ggplot}
#GGPLOT() gives you even more options for adding layers/options
p <- ggplot(mtcars, aes(x = mpg, y = wt))
p + geom_point()

#now play time with this case study.
#try out some geom options and different aesthetics and make some errors.
#prize for the prettiest plots

#displ is car engine size in Liters
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

#so aethetics are one way to add variables in and expand your plotting power
#however facets are another way to make multiple plots BY a factor

#facet wrap is by one variable
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

#facet_wrap(~cell) - univariate: create a 1-d strip of panels, based on one factor, and wrap the strip into a 2-d matrix
#facet_grid(row~col) - (usually) bivariate: create a 2-d matrix of panels, based on two factors

#facet grid is by two variables
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)

#another example more perfect code
p <- ggplot(data = mpg, aes(x = displ, y = hwy, color = drv)) + geom_point()
p + facet_wrap(~cyl)

#or just use facets in qplot but much simpler
qplot(displ, hwy, data=mpg, facets = . ~ year) + geom_smooth()

```

<b>Data viz case study #4.</b>
```{r, worked example}
#try it with ecological.footprints.csv
footprints <-read.csv("data/ecological.footprints.csv")
str(footprints)
#aha, R thinks year is an integet
footprints$yr <- as.factor(footprints$yr)
library(ggplot2)
qplot(country, ecological.footprint, data = footprints) #too messy

qplot(country, ecological.footprint, data = footprints, colour = yr) #better but still a lot to process

qplot(country, ecological.footprint, data = footprints, facets = yr~.) #ok but do not love. hard to see distribution

qplot(ecological.footprint, data = footprints) #you know what, this is not bad.  maybe add year in too/

qplot(ecological.footprint, data = footprints, fill = yr) #ok now I starting to see structure and differences

#OK, now I am ready for stats. Thinking about these data, I see we have only two years for some countries so cannot do within country or between country contrasts. So, most likely hypothesis I can test is whether ecological footprints are increasing between these two years. Not a perfect dataset really but could compare these two years.

t.test(footprints$ecological.footprint~ footprints$yr)
#ok looks like there are differences between years but it is hard to tell from previous plot. Realize now, I need a better plot still?

qplot(yr, ecological.footprint, data = footprints, geom="boxplot") #this is weird, 2000 looks higher

#different countries between years?
#more countries reported in 2000?
library(dplyr)
footprints %>% count(yr)
#Yup, way more data for year 2000
#maybe should just the countries that were testedin both years?

library(tidyr)
matches <- spread(footprints, yr, ecological.footprint) %>% filter() %>% na.omit
str(matches)
matches

#whoa, USA and Canada missing, and we have HUGE footprints.

#got it. just the countries with measure in BOTH years.
#now, gather up again with these filtered matches!

new <- matches %>% gather(`2000`, `2012`, key = "yr", value ="ecological.footprint")
new #ok so now a nice dataframe with just matches back in a format I can use for plots and stats

qplot(yr, ecological.footprint, data = footprints, geom="boxplot")

t.test(new$ecological.footprint~ new$yr, paired=TRUE)
#Well, I give up, seems like the footprints for the world went down not up in this time period. GOOD NEWS for the environmental movement in some respects.
new %>% group_by(yr) %>% summarise(mean(ecological.footprint))

# We still use 2.4 planets worth of resources but only have one.

#AND, we are missing some key countries that contribute to global change including Canada and USA.

```

<b> Practical skill outcomes of R stats useful for competency test</b>
Do meaningful plot of a single variable wth nice labels.
Do a meaningful plot of a relationship between two variables.
Use qplots to do a plot that includes an aesthetic.
Do a ggplot that includes three variables.

[Physics example in Wired Mag why to plot data even if you have a model](https://www.wired.com/2016/09/might-gotten-little-carried-away-physics-time/)
 
###Lesson 4. Exploratory data analysis (EDA) in r.
![](./EDA.png)

fundamental descriptive stats.
distributions via density plots
GLM. GLMM. Post hoc tests.
modelR

[slide deck for EDA](http://www.slideshare.net/cjlortie/exploratory-data-analysis-and-models-in-r)

<b> Philosophy of R stats </b>
Exploratory data analyses is everything we have done.
a. Tidy data.
b. Inspect data structure.
c. Data viz.
d. Basic exploratory data analyses.

However, now that we are ready to apply models, we add in one more tiny step. Visualize the data to better understand its typology and underlying distribution.  Then, you are ready to fit your models.

A statistical model is an elegant, representative simplification of the patterns you have identified through data viz and EDA. It should capture data/experimental structure including the key variables, appropriate levels, and relevant covariation or contexts that mediate outcomes. It should support the data viz. It should provide an estimate of the statistical likelihood or probability of differences. Ideally, the underlying coefficients should also be mined to convey an estimate of effect sizes. A t.test, chi.square test, linear model, general linear model, or generalized linear mixed model are all examples of models that describe and summarize patterns and each have associated assumptions about the data they embody. Hence, the final step pre-model fit, is explore distributions.

Conceptually, there are two kind of models. Those that look back and those that look forward. Think tardis or time machine. A model is always a snapshot using your time machine. It can be a grab of what happened or a future snap of what you predict. In R, there is simple code to time travel in either direction. Actually, there is no time - [Arrow of time](https://www.wired.com/2016/09/arrow-of-time/) - only an observer potential perception of it. Statistical models are our observers here. These observers use 'probability distributions' as we described in the first week sensu statistical thinking to calibrate what the think we observed or will observe given the evidence at hand.

<b>Case study #1: x,y continuous</b>
```{r, case study 1}
library(ggplot2)
library(dplyr)
library(modelr)
#Data viz for pattern
qplot(x,y, data = sim1)

#Now inspect distribution of y
ggplot(sim1) + 
  geom_density(mapping = aes(y))

ggplot(sim1) + 
  geom_histogram(mapping = aes(y), binwidth=5)

shapiro.test(sim1$y) #p-value <0.05 means is different from normal
qqnorm(sim1$y) #qqplots common to inspsect quantiles

#not significantly different from normal. great.
#model time!

#Remember, you own the purpose! Does x predict y? Linear model straight up!

m1 <-lm(y~x, data = sim1)
summary(m1)
#wow, these simulated are too good. Remember, this is the backwards, time travel, hypothesis test given what we have. Not prediction of new data per se but description of observation of patterns.
coef(m1) #remember 'minecraft' your model a bit to get a sense of effects.

```

![](./families.png)

<b> Case study #2 categorical x, continuous y</b>
```{r, case study study}
ggplot(sim2) + 
  geom_point(aes(x, y)) #categorical x

ggplot(sim2) + 
  geom_density(mapping = aes(y))

ggplot(sim2) + 
  geom_histogram(mapping = aes(y), binwidth=1) #changing binwidth really changes perception of distribution

shapiro.test(sim2$y) 
qqnorm(sim2$y) #non-normal
#so, could do anova but likely not great link to underlying probability distribution
m2 <- aov(y~x, data = sim2)
summary(m2)
names(m2)
plot(m2$residuals) #residuals not bad
#library(fitdistrplus)

m3 <- glm(y~x, data = sim2, family = "quasi")
summary(m3) #better
#you can also explore distributions in even more detail to ensure correct model (correct = matches/describes underlying structure AND data distribution)
anova(m3, test="Chisq") #is x significant factor?

#Cullen and Frey Graphs are cool
#fitdistrplus not bad.  Usually, the type of data, ie. count, frequency, proportion is just as effective on deciding on family type vesus distribution exploration.  The goal is to fit the 'best' model. Best is simplest and representative. Formal tools to contrast models sometimes help too.

#note, mixed models, can use lme4 package if some effects and others are random. Need to think this over. Fixed = groups or levels in factors not due to random causes, random effects = likely from random causes or latent drivers such as population/species specificity.

```

<b> Case study #3: interactions cat.x, cont.x, y</b>
```{r, interactions with different x.classes}
str(sim3)
ggplot(sim3) +
  geom_point(aes(x1, y, colour = x2))
#x1 is continous
#x2 is categorical
#Q are there an effect of xs on y and do the effect interact, i.e. level of x1 influence changes by x2.

ggplot(sim3) + 
  geom_density(mapping = aes(y))

ggplot(sim3) + 
  geom_histogram(mapping = aes(y), binwidth=1)

shapiro.test(sim3$y) 
qqnorm(sim3$y)  
#s.d. from normal but not bad. could be contigent on the levels of x.

ggplot(sim3, aes(x1, y)) + 
  geom_boxplot(aes(colour = x2))
#so, looks like the distribution of y relates to the factors. Likely good to go on parametric linear model.

m4 <-lm(y~x1*x2, data = sim3) #interactions terms for all levels
summary(m4)
m5 <-lm(y~x1+x2, data = sim3) #independent x1 & x2 effects on 7 modeled.
summary(m5)
plot(m5) #sometimes I plot the model to explore/mine model for its capacity to describe the patterns
anova(m5, test="Chisq") #tells you if effects are significant.

#you can also contrast different models using the anova of different models.
m.b <-anova(m4,m5, test="Chisq")
m.b

```

<b> Case study #4: interactions cont.x, cont.x, y</b>
```{r, interactions with same x.classes}
str(sim4)
ggplot(sim4) +
  geom_point(aes(x1, y, colour = x2))

ggplot(sim4) + 
  geom_density(mapping = aes(y))

ggplot(sim4) + 
  geom_histogram(mapping = aes(y), binwidth=1)

shapiro.test(sim4$y) #more divegence from normality
qqnorm(sim4$y)  
m6 <-glm(y~x1*x2, data = sim4)
summary(m6) #significant interaction term in model
anova(m6, test="Chisq") #Looks solid.

m7 <-glm(y~x1+x2, data = sim4)
summary(m7) #missed interaction term here and given distribution exploration, likely important.

m.b2 <-anova(m6, m7, test="Chisq")
m.b2

```

<b> Case study #5: real data diamonds</b>
```{r, diamonds you never fail use}
#Lucy.
#See data viz week for first steps.
#real data always more complex
ggplot(diamonds) +
  geom_point(aes(carat, price, colour = cut))
#so price is the most likely response we want to know about!
#two key factors, different x.classes, cut is categorical, and carat is continous
#look at price distribution (likely need to do by each x)
ggplot(diamonds) +
  geom_freqpoly(aes(price)) #long tail

#now do by the two xs to see how y varies
ggplot(diamonds, aes(carat, colour = cut)) +
         geom_freqpoly(binwidth = 0.1)

set.seed(1000)
dsmall<-diamonds[sample(nrow(diamonds), 1000), ]
shapiro.test(dsmall$price) 
qqnorm(dsmall$price)  
#ok, so we have a handle on the distribution.
#certainly non-normal.

#last idea!
#before we move to fitting a model recognizing that the data look like negative binomial or poisson given the class, what if there are a relationship between the the xs?
#what if larger diamonds cost more and better cut diamonds costs more but there are not more better cut AND large diamonds out there? Covariation is not positive.

#EDA on just that
ggplot(diamonds, aes(cut, carat)) +
  geom_boxplot()
m8 <-glm(carat~cut, data=diamonds)
summary(m8) #looks different! More poor cut diamonds are larger...
library(lsmeans)
lsmeans(m8, "cut", adjust="tukey") # to see ls means

#ok now ready for a simple model.
library(MASS)
m9 <-glm.nb(price~carat*cut, data = diamonds)
summary(m9)
anova(m9, test="Chisq")

```
<b>Tips</b>
lme4 for mixed models
vegan for ordinations
Lavaan for SEMs
MASS for count and negative binomial data
(1 | factor) treats as random factor

Ensure you see the different applications of the following models:
anova
lm
glm
glmm

[EDA from a data science perspective: predictive](http://r4ds.had.co.nz/exploratory-data-analysis.html#introduction-3)

<b> Practical skill outcomes of R stats useful for competency test</b>
Worflow description complete now.

Be able to use EDA & data viz to select a model.
Be able to explore distributions of datasets.
Be able to fit descriptive models (super simple to simple).
Predictive models if you like too but not required.
Be able to examine efficiacy of a model.

Recognize through application of a few models that the following rule is never broken in stats...

Rule: Statistics are never prescriptive. 

Processes include description or prediction.
Models are powerful, purposeful tools you can use to capture & communicate evidence.

<b>Homework</b>
Revisit survey, ttc, or footprints data and end with a model.

<b> Additional readings </b>

[GLMM for ecologists](http://glmm.wikidot.com)

[A practical guide to linear models](http://avesbiodiv.mncn.csic.es/estadistica/curso2011/regm26.pdf)

[General how to choose the right test](http://med.cmb.ac.lk/SMJ/VOLUME%203%20DOWNLOADS/Page%2033-37%20-%20Choosing%20the%20correct%20statistical%20test%20made%20easy.pdf)

[SUPER flowchart](http://abacus.bates.edu/~ganderso/biology/resources/stats_flow_chart_v2014.pdf)

[Interpreting R output](https://rstudio-pubs-static.s3.amazonaws.com/119859_a290e183ff2f46b2858db66c3bc9ed3a.html)

![](./flowchart.jpg)


###Lesson 5. Wrangle, visualize, and analyze.
![](./halloween.jpg)


Here is a webinar I like and also a good book chapter.

[Tutorial on reading data into R](https://www.rstudio.com/resources/webinars/getting-your-data-into-r/)

[Great read on efficient data carpentry](https://bookdown.org/csgillespie/efficientR/data-carpentry.html)

[Efficient statistics slide deck](http://www.slideshare.net/cjlortie/efficient-statistics-in-r)

<b>Halloween Hackathon</b>
Costumes options.
Candy provided.
Now, we practice from start to finish including submission of the r-script or rmd you write to turnitin.com

A graduate-level dataset.
Apply your new mathemagical skills from scratch.
A single three-hour block.
As advanced as you want to be.
Fundamental principles demonstrated.
At least two fundamental statistical tests demonstrated.

<b>Practical outcomes to demonstrate </b>
Rename some variables.
Address missing values.
Generate a simplified table/dataframe. 
Visualize the data to identify patterns and relationships.
Produce a publication quality graphic.
Do EDA on data very broadly.
Do a single statistical address to capture main effect observed.
Annotate

<b>Rubric</b>
A total of 25% so 5 questions each worth 5 points.
Likert Scale of 1 to 5 with 1 being low and 5 being awesome.

(1) Can I understand what was done?
(2) Can I repeat what was done?
(3) Does the data viz effectively and thoroughly examine and show patterns/relations?
(4) Is the EDA a clear and appropriate examination the evidence and demonstrates statistical thinking?
(5) Is the final graphic and statistical test appropriate (tidy, polished, and meaningful) and suitable for publication?

###Lesson 6. Competency test.
![](./test.jpg)

[General best practices on scientific computing](https://swcarpentry.github.io/good-enough-practices-in-scientific-computing/)

[Cheatsheets!](https://www.rstudio.com/resources/cheatsheets/)

I recommend printing them up.
Also, scroll down on page, the regex one is great. Lists the gsub function.

deliberate practice.
tested with feedback.
a virtual repeat of last week with new data but evaluated.
Pizza provided this week.

<b>Prep suggestions</b>
1. Learn how to use inner_join
2. Practice using the 'file/knit document' within RStudio. [I am happy to review and accept only the R code but as as backup a knit of the output including code as PDF is good. Also, knitted docs/pdfs/html pages are super for sharing your analyses with your graduate researcher collaborators or mentors]. This entire 6-week course is just a single RMD file knitted. 



